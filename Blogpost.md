# GNNInterpreter Reproducibility Study

#### Ana-Maria Vasilcoiu, Batu Helvacioğlu, Thies Kersten, Thijs Stessens

---

This blog post aims to offer readers a brief overview of our research methodology and findings, with the complete report accessible here: https://openreview.net/forum?id=8cYcR23WUo. 

## Introduction
Graph Neural Networks have recently gained recognition for their performance on graph
machine learning tasks. The increasing attention on these models’ trustworthiness and
decision-making mechanisms has instilled interest in the exploration of explainability techniques, including the model proposed in ["GNNInterpreter: A probabilistic generative modellevel explanation for Graph Neural Networks. (Wang & Shen (2022))"](https://arxiv.org/abs/2209.07924).

This repository contains the official implementation for the reproducibility study conducted based on the mentioned paper. The study aims to examine and validate the results demonstrated by Wang & Shen (2022) through reproduction of their experiments. Our **contributions** can be summarized as follows:
1. Through an analysis on the training dynamics of GNNInterpreter, we demonstrate that the method performs best on large graphs, but experiences training instability when working with small graphs
2. We also find evidence that larger graphs tend to produce more faithful and realistic results.
3. We show that GNNInterpreter requires hyperparameter tuning with domain-specific knowledge or searching random initializations to generate faithful and realistic explanations when used with datasets that have small graphs.
4. We show evidence that GNNInterpreter can be used as a general approach with different types of node and edge features.
5. Based on our qualitative analysis, we show that the explanations generated by GNNInterpreter are generally on par with XGNN and are outperformed by GNNExplainer for certain datasets.
6. We show empirically that the time complexity of GNNInterpreter is much lower than XGNN, but that in practice the time spent for hyperparameter tuning or random seed searching should be accounted for.

## GNNInterpreter Model

GNNInterpreter is a model-agnostic and model-level explanation method aiming to reveal the high-level decision-making process of message passing based GNNs. It works by learning a probabilistic generative graph distribution in order to produce the most discriminative graph pattern that the explained GNN detects when making a prediction.

#### Learning objective

GNNInterpreter achieves its goal by numerically optimizing a novel objective function
with two distinct but important parts:
1. maximizing the likelihood of the explanation graph to be predicted as the target class by the GNN model.
2. confining the explanation graphs distribution within domainspecific boundaries.

Mathematically, the learning objective can be formulated as follows:

$$\begin{align} 
\max _ {G} L(G)=\max _ {A, Z, X} L(A, Z, X)=\max _ {A, Z, X} \phi_{c}(A, Z, X)+\mu \cdot \text{sim}_ {\cos }\left(\psi(A, Z, X), \bar{\psi}_{c}\right) & \qquad \qquad
\end{align}$$

#### Graph distribution

To learn a probabilistic graph distribution, two assumptions are made: (1) the explanation graph is a Gilbert random graph and (2) the features are independently distributed, allowing for the following factorization of a random graph variable G.

#### Continous relaxation

To address the discrete nature of graphs and achieve node and edge feature
flexibility, GNNInterpreter uses continuous relaxation of the discrete variables to continuous variables which are then specified using the Concrete distribution. The random variables are modeled to be continuous as follows:

$$\begin{align} 
\tilde{x}_ {i} \sim \text{Concrete}\left(\xi_{i j}, \tau_{x}\right) \quad for \quad \tilde{x}_ {i} \in \tilde{\mathbf{X}} \quad and \quad \xi_{i} \in \boldsymbol{\Xi} & \qquad \qquad
\end{align}$$

$$\begin{align} 
\tilde{z}_ {i j} \sim \text{Concrete}\left(\eta_{i j}, \tau_{z}\right) \quad for \quad \tilde{z}_ {i j} \in \tilde{\mathbf{Z}} \quad and \quad \eta_{i j} \in \mathbf{H} & \qquad \qquad
\end{align}$$

$$\begin{align} 
\tilde{a}_ {i j} \sim \text{BinaryConcrete}\left(\omega_{i j}, \tau_{a}\right) \quad \quad for \quad \tilde{a}_ {i j} \in \tilde{\mathbf{A}} \quad and \quad \omega_{i j} \in \boldsymbol{\Omega}& \qquad \qquad 
\end{align}$$

The reparameterization trick is also applied to make this distribution differentiable, facilitating gradient-based optimization.

An independent random variable ϵ ∼ Uniform(0, 1) to adjust the sampling function as follows:

$$\begin{align} 
\tilde{x}_ {i} \sim \text{Softmax}\left(\left(\xi_{i}-\log (-\log \epsilon)\right) / \tau_{z}\right) & \qquad \qquad 
\end{align}$$

$$\begin{align} 
\tilde{z}_ {i j} \sim \text{Softmax}\left(\left(\eta_{i j}-\log (-\log \epsilon)\right) / \tau_{z}\right)  & \qquad \qquad 
\end{align}$$

$$\begin{align} 
\tilde{a}_ {i j} \sim \text{sigmoid}\left(\left(\omega_{i j}-\log \epsilon-\log (1-\epsilon)\right) / \tau_{a}\right) & \qquad \qquad 
\end{align}$$

With these two modifications it’s possible to draw samples, that correspond to the distribution, without losing differentiability. The learning objective is then approximated using the Monte Carlo method with K samples:

$$\begin{align} 
\max _ {\Theta, Q, P} \mathbb{E}_ {G \sim P(G)}[L(A, Z, X)] \approx \max _ {\Omega, H, \Xi} \mathbb{E}_ {\epsilon \sim U(0,1)}[L(\widetilde{A}, \widetilde{Z}, \widetilde{X})] \approx \max _ {\Omega, H, \Xi} \frac{1}{K} \sum_{k=1}^{K} L(\widetilde{A}, \widetilde{Z}, \widetilde{X}) & \qquad \qquad 
\end{align}$$

#### Regularization

The model uses several types of regularization:
1. L1 and L2 regularization are used to avoid gradient saturation and to also encourage model sparsity.
2. A budget penalty is used to limit the size of the explanation graphs and to prevent them from growing indefinitely with repeated discriminative patterns.
3. A connectivity incentive term is applied to promote correlation, which is done by minimizing the Kullback-Leibler divergence between the probabilities of each pair of edges that share a common node.


## Methodology

The GNNInterpreter implementation, alongside with datasets, pre-trained classifiers and experiment notebooks is publicly available Consequently, we make use of the authors’ code, in 2 different versions, namely the initial and the latest version, and with some minor bugfixes and improvements. Additionally, we enhance and extend the work of Wang and Shen by conducting additional experiments and presenting further results on the reddit-binary dataset.

### Datasets
The experiments conducted in this study use 5 different datasets (first 4 similar to the original paper, while the last represents further experimental analysis in this study):
1. Synthetic datasets:
    1. **Shape** dataset consists of 5 classes of graphs: Lollipop, Wheel, Grid, Star and Others. Each class contains graphs with the corresponding shape, while the Others class has graphs with random topology.
    2. **Motif** dataset classifies graphs by motifs like House, House-X, Complete-4, and Complete-5, each with its own label. It also includes a class for graphs without unique motifs. Nodes in this dataset have a categorical feature with 5 color values.
    3. Graphs in the **Cyclicity** dataset have edges with either green or red labels. They are classified into three categories: Red-Cyclic, Green-Cyclic, and Acyclic. Red/Green-Cyclic graphs contain cycles made solely of red or green edges, while Acyclic graphs lack cycles or have cycles with both red and green edges.
2. Real-world datasets:
    1. The [**MUTAG**](https://paperswithcode.com/dataset/mutag) dataset consists of graphs representing molecules labeled as either mutagenic or nonmutagenic. The dataset's original paper highlights factors affecting mutagenic properties, including molecule hydrophobicity, energy levels, and ring fusion. For analysis, we consider the presence of NO2 bonds and fused rings as indicating mutagenicity, although determining mutagenicity is complex.
    2. The [**Reddit-Binary**](https://paperswithcode.com/dataset/reddit-binary) dataset comprises of real-world text post examples and their associated comments, forming threads from two Reddit post types. The posts are represented as graphs where nodes represent users and edges signify interactions, like comments or replies. They are divided into two classes: question-answer threads from IAmA and Askreddit, and discussion threads from TrollXChromosomes and atheism.

### Experimental set-up

To validate the accuracy of the base GNN models, we tested both the provided
model checkpoints and retrained models. Furthermore, we measured the performance of the GNNInterpreter both quantitatively and qualitatively like in the original paper. Lastly, we did some further analysis on the training dynamics after encountering instability during training, by tracking the explanation graph size and correct class probability throughout training iterations. 


### Results and Discussion

This study aimed to verify the following 4 claims:
1. **Claim 1**: The explanations generated by GNNInterpreter are faithful and realistic. Additionally, GNNInterpreter doesn’t require domain-specific knowledge to achieve that.
2. **Claim 2**: GNNInterpreter is a general approach that performs well with different types of node and edge features.
3. **Claim 3**: The explanations generated by GNNInterpreter are more representative regarding the target class compared to XGNN.
4. **Claim 4**: The time complexity for training GNNInterpreter is much lower than for XGNN.

Regarding the realism and faithfulness of generated explanations, we have found that
adjusting hyperparameter values yielded mixed quantitative and qualitative results across datasets, with a significant sensitivity to seed variation. While the technique holds promise for generating realistic results, its unreliability poses a significant challenge to achieving those in practice. During our experiments, GNNInterpreter was never able to reliably achieve realistic results for some classes, indicating that its performance is also heavily influenced by the dataset and class.

We found the main reason of the training instability to be the graph size of the dataset. Even though we use relaxation to model the graphs as continuous distributions, the loss behaves discretely when the graph size is small. The instability in GNNInterpreter's training may also be attributed to dynamic weighting of the budget penalty and the arbitrary selection of an early stopping criterion, design decisions not mentioned in the original paper. These factors had a significant impact on model performance, sometimes aiding convergence and other times leading to non-convergence. Both depended on the max number of nodes parameter, which was not originally discussed and requires fine-tuning for each dataset. We think that removing early stopping could potentially mitigate non-convergence issues.

As for the faithfulness of explanation graphs, we found evidence that using this technique on a new problem doesn't guarantee that the explanations are representative even after proper fine-tuning. Overall, we found that this method requires domain knowledge for tuning parameters, and is unstable with datasets that have small graphs. This shows that **Claim 1 holds for datasets with large graphs** where the impact of a single node on the classification outcome is relatively small. **Claim 1 does not hold however for datasets with smaller graphs**. For these datasets, faithful explanations can reliably generated only by extensive parameter tuning with domain knowledge or trying random initializations.

Moreover, by being able to train at least some good interpreters on multiple datasets, each with unique types of node features, our results provide **evidence in support of Claim 2**. 

Despite the limitation of solely experimenting with XGNN on the MUTAG dataset, our results provide sufficient basis for drawing meaningful conclusions about Claim 3 and 4. **For the MUTAG dataset our results contradict Claim 3**. 

While we were able to replicate the training times for a single
model as reported by the authors, it is worth noting that for most datasets, we had to train on multiple seeds (as many as 20) to obtain a single good interpreter. As such, we believe that the training times should better reflect the actual amount of time required to achieve a good model either by trying random seeds or tuning parameters and sampling larger graphs for complicated classes. Nevertheless, for a single model, we have found GNNInterpreter to be nearly 38 times faster than XGNN and we thus affirm the **validity of Claim 4 on the MUTAG dataset**.

## Concluding remarks

We believe GNNInterpreter could be improved to be more reliable, stable, and require less domain knowledge at the cost of increased training times. Using a max class size for sampled graph size could eliminate the never-converging problem, reducing reliance on random initializations as well as domain knowledge. Exploration-exploitation trade off could be better balanced by carefully selecting max number of nodes, and saving the best model.
